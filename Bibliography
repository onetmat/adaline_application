Bibliography of Mathew Anderson, CS358 -


1) R. J. Meuth, ``GPUs Surpass Computers at Repetitive Calculations,'' \emph{IEEE Potentials}, Nov/Dec, 2007.
	Summary of articles that have proved how utilization of GPUs on video cards can improve algorithm performance. Gives brief overview of major CI fields of study touched on by Wunsch and how they might also be improved by GPU utlization.

2) J. H. Kim, et al., ``Intelligence Technology for Robots That Think,'' \emph{IEEE Compt. Intell. Mag.}, Aug, 2013.
	Described iOA (Intelligence Operating Architecture) used to bring all forms of Robot Intelligence (initial focus of article) together. Gave specific examples of each type of intelligence, offering demonstrations of concepts through simulation. Provided multiple references to source math and brief mathematical overview of each application.

3) D. Binkley, "Source Code Analysis: A Road Map,'' in \emph{Future Software Engineering}, Minneapolis, MN, 2007.
	Provides an overview of current source code analysis techniques and identifies current challenges prohibiting further analysis. Overview provides a large number of references for further reading. Section two in particular provides interesting detail on particular source code analysis techniques, but not necessarily applications. Identifies three debugging techniques of particular interest and provides references for further reading.

4) W. Blewitt, et al., ``Applicability of GPGPU Computing to Real-Time AI Solutions in Games,'' \emph{IEEE Trans. Comput. Intell. AI Games}..., to be published.
	Summarizes major advances in algorithmic processing due to GPGPU applications throughout the gaming community, focusing on AI relevant to games. Briefly reviews comparisons of OpenCL, CUDA, and DirectCompute. Emphasized enhancements to Path finding, other general searching, Finite State Machines, and Line of Sight/Detection. Provides a dearth of references for addtional reading in all cases.

5) C. M. Grinstead and J. Snell, ``Markov Chains,'' in \emph{Introduction to Probability}, 2nd ed. Boston, MA: FSF, 2006, ch. 11, sec. 1, pp. 405-416.
	Introduction to Markov Chains with exercises and examples. Particular focus on matrix representation. Complete book available online at <http://http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html>, retrieved on 25 August, 2013.

6) P. Werbos, ``ADP: Goals, Opportunities and Principles,'' in \emph{Learning and Approximate Dynamic Programming}, New York, NY: Wiley, 2004, ch 1, pp. 1-37.
	Description of ADP's application to reality and mathematical underpinnings. Defines reinforcement learning as maximization of a utility function over time, discounted by a particular value each iteration (clock tick, or value at a pre-selected time index). Many references to the work of John Von Neumann and first generation contemporaries discussed. Real-world examples include the "Clean Car" chip developed for the Ford Motor Company and efficient energy production, such as Space Solar Power. Limitations of straight dynamic programming and how approximation can help are discussed in detail, with clear definitions and examples of DP. Large portion of the second half are devoted to the Bellman equation.

7) P. Werbos, ``Backpropagation Through Time: What It Does and How to Do It,'' \emph{Proc. IEEE}, vol. 78, pp. 1550-1560, Oct. 1990.
	Full mathematical definition of Backpropagation Through Time with examples. Walks through all of the calculus required to understand what's going on, gives code examples of each piece, and puts all concepts together. Slow pace and dense reading. Referenced a book called "Parallel Distributed Processing" which, "...played a pivotal role in the development of the field."

8) Michael Fairbank, et al., ``Simple and Fast Calculation of the Second-Order Gradients for Globalized Dual Heuristic Dynamic Programming in Neural Networks,''  \emph{IEEE Trans. Neural Netw.}, vol. 23, pp. 1671-1676, Oct. 2012.
	 Describes an efficient method for forward propagating error, which apparently opposes backpropagation. No reference to Werbos' Backpropagation Through Time article is made, but several other Werbos articles are cited. The article mentions specifically that the algorithm being described refines the error function of a neural network. The authors offer psuedocode implementations and sample output as proof of their work. The focus shifts to particular ADP algorithms and how their second order derivative calculation algorithm reduces runtime complexity. This article reads almost like a Wikipedia article and many sources will have to be consulted. Referneced an article by F.-Y. Wang, H. Zhang, and D. Liu called "Adaptive dynamic programming; An introduction'' from IEEE Comput. Intell. Mag. (first reference). Probably worth a look.

9) Bernard Widrow, et al., ``Punish/Reward: Learning with a Critic in Adaptive Threshold Systems,'' \emph{IEEE Trans. Syst. Man Cybern.}, vol. SMC-3, pp. 455-465, Sep. 1973.
	Paper describes a physical punishment/reward learning circuit. Contains a complete example of teaching the circuit to play a simplified form of Blackjack. This paper lays an excellent foundation for reinforcement learning and provides a ground-up example that includes most aspects of integration, etc. All of the math involved is laid out and it gets a little intense at times.

10) M. A. Boyacioglu, et al., ``Predicting bank financial failures using neural networks, support...,'' \emph{Expert Systems with Applications}, 2008.
	Demonstrates use of multiple neural network models, and other techniques, to predict a bank failure. The opening sentence of section five most succinctly explains the methods employed. In the neural network case, the historical data used to train the networks comes from the Savings Deposit Insurance Fund of the country of Turkey.

11) S.E. Taylor, et al., ``Temporal Semantics: An Adaptive Resonance Theory Approach,'' in \emph{Proc. of Int. Joint Conf. on Neural Netw.}, Atlanta, GA, 2009, pp. 3111-3117.
	Articles describes an attempt to incorporate time into neural networks. The opening discussion about why is particularly worth reading. In short, memory, human and computer, has more meaning when associated with a persistent time model.

12) B. Widrow and M.E. Hoff, Jr., ``Adaptive Switching Circuits,'' IRE WESCON Convention Record, 4:96-104, August 1960.
	Birth paper of Adaline. Paper provides a mathematical model for an adaptive ciruit, describes Adaline, and gives some sample applications.

13) B. Widrow, J. McCool, and M. Ball, ``The Complex LMS Algorithm,'' Proceedings of the IEEE, 63(4):719-720, April 1975.
	Derives and demonstrates how the LMS algorithm can be applied to complex numbers.

14) B. Widrow, ``Adaline: Smarter than Sweet,'' Stanford Today, Autumn 1963.
	Article provides a brief overview of several applications of Widrow's Adaline. The article briefly describes severtal research efforts at Stanford University utilizing Adaline circuits.

15) B. Widrow, ``Adaptive Sampled-Data Systems --- A Statistical Theory of Adaptation,'' IRE WESCON Convention Record, 4:74-85, 1959.
	This paper details the mathematical underpinnings of Adaline, but was published before the ``Adaptive Switching Circuits''. A lot of material from ``Adaptive Switching Circuits'' seems to have its origins in this paper.

16) B. Widrow, ``Statistical Analysis of Amplitude-Quantized Sampled-Data Systems,'' AIEE Transactions on Applications and Industry, pp.1-14, January 1961
	Defines Quantization and provides statistical analysis of quantizing a signal. Also details methods of dealing with the noise of physical quantizing units.

17) B. Widrow, ``Thinking About Thinking: The Discovery of the LMS Algorithm,'' IEEE Signal Processing Magazine, 22(1):100-106, January 2005.
	Retrospective view of the LMS Algorithm's discovery. This paper reads like a general summary of ``Adaptive Switching Circuits''.

18) D. Rumelhart, B. Widrow and M.A. Lehr, ``The Basic Ideas in Neural Networks,'' Communications of the ACM, 37(3):87-92, March 1994.
	Defines a neuron in the context of an artifical neural network. Article also briefly describes backpropagation and other mehods used to train neural networks. It ends with a most interesting section called, "Hints for Successful Applications", specifically stating that careful input modeling - authors recommend "forward" probabilistic models - is key to use of neural networks to solve a problem.

19) B. Widrow, I. Kollar, and M.-C. Liu, ``Statistical Theory of Quantization,'' IEEE Transactions on Instrumentation and Measurement, 45(2):353-361, April 1996.
   Parallels sampling with quantization. Specifically highlights floating-point quantization. Article ties in with previous ADALINE articles by giving a brieft overview of quantization theory.

20) R. Winter and B. Widrow, ``MADALINE RULE II: A Training Algorithm for Neural Networks,'' Proceedings of the IEEE International Conference on Neural Networks, 1:401-408, July 1988.
   Presents an algorithm for employing a multiple ADALINE (MADALINE) based neural network. The article, almost conspicuously, lacks a mathematical model and the algorithm's proof seems entirely empirical. Authors mention work being done a mathematical proof. Contains an interesting discussion of how to avoid stagnation of the "Generalization" phase of a neural network. Perhaps most important to note - Learning is when you are training the MADALINE system, Generalization is when it is being presented with patterns outside the training set.

21) D. Nguyen and B. Widrow, ``The Truck Backer-Upper: An Example of Self-Learning in Neural Networks,'' Proceedings of the International Joint Conference on Neural Networks, 2:357-362, 1989.
   Details a sample application of a MADALINE system. Authors detail how they model a tractor-trailer (semi-truck) and its approach to a loading dock. They empirically prove that, given certain assumptions such as distance from the loading dock, a neural network can solve complicated tasks when properly modeled.

22) B. Widrow and M.A. Lehr, ``Perceptrons, Adalines, and Backpropagation,'' in Handbook of Brain Theory and Neural Networks, M.A. Arbib, ed., pp.719-724, MIT Press, 1995.
   Overview of the perceptron learning method, the LMS learning method, and backpropagation. Various references to more or less practical applications (the Truck Backer-Upper being one of them) are included. 

23) Widrow, ``Generalization and Information Storage in Networks of Adaline `Neurons','' in Self-Organizing Systems, symposium proceedings, M.C. Yovitz, G.T. Jacobi, and G. Goldstein, eds., pp.435-461, Spartan Books, Washington, DC, 1962.
   Discusses the memory of an ADALINE and introduces MADALINE (Multiple ADALINE) systems. Of particular interest, at the very end of section B, the paper asserts that due to findings from its source material, "...the average number of random patterns that can be absorbed by an ADALINE is equal to twice the number of weights." Memory of an ADALINE in this case, refers to a physical implementation of an ADALINE and the paper goes on to describe phsyical ADALINEs as collections of memistors (one memistor per input line). The other side of the juxtaposition is traditional core memory. Paper also discusses how noise in the input signals can affect the Learning phase.

24) B. Widrow, et. al, ``Practical Applications for Adaptive Data-Processing Systems,''IRE WESCON Conference Record, pp.1-9, 1963. Reprinted in Rev. A, 10:27-39, 1968.
   Article describes a few realized applications of ADALLINE based systems. Starting in section F, the paper discusses the issues surrounding proper quantization and coding of input signals. Finishes with a brief overview of using memistors as input lines for physical ADALINEs and how physical ADALINEs could be mass produced in this way.

25) B. Widrow and M.A. Lehr, ``Feedforward Networks,'' in INNS Above Threshold, December 1992.
   Gives brief into to ADALINE and presents LMS and backpropagation learning approaches. Brief and succinct summary of Widrow's contributions to CI; easy to read.

26) K.J. Hunt, et al., ``Neural Networks for Control Systems - A Survey,''  Automatica, vol. 28, pp. 1083-1112, 1992.
	Reviews various forms of neural networks; includes a "Summary of Architectures" on page 1092 giving a summary of the first section. The second section of the paper discusses applications of neural networks with respect to non-linear control systems. Reviews of the architectures are in-depth and contains a derivation of, for example, a backpropagation architecture.

27) Rajneese Sharma and Matthijs T. J. Spaan, ``Bayesian-Game-Based Fuzzy Reinforcement Learning Control for Decentralized POMDPs,'' \emph{IEEE Trans. Comput. Intell. AI Games}, vol. 4, pp. 309-328, Dec. 2012.
   Article studies how a collection of Agents (instantiations of an artifically intelligent machines) could overcome noise of sensors, etc, using a Markov decision process. Specifically, they study a Partially Observable Markov decision process (or POMDP in the paper) and incorporate a fuzzification system where each agent knows a particular piece of information (a Q variable) that may solve another agent's confusion of imperfect sensory input.

28) Mark de Berg, et al., ``Quadtrees,'' in \emph{Computational Geometry: Algorithms and Applications}, 3rd ed. Berlin, Germany: Springer, 2008, ch. 14, pp. 307-322.
   Overview of Quadtree. Provides mathematical definition, demonstrates uses, provides algorithms for construction. Highlights key insights, such as inability to bound a Quadtree by the number of points in the underlying dataset.

29) Hiroshi Tsukimoto, ``Extracting Rules from Trained Neural Networks,'' \emph{IEEE Trans. Neural Netw.}, vol. 11, pp. 377-389, Mar 2000.
   Article describes how to extract "rules" (logical propositions) from trained neural networks using binary logic in both the continuous and discrete domains. Author defines terminology in ways inconsistent with conventional mathematics in some cases. Examples include discerning political affiliation from voting record and classifying mushrooms based on pre-defined categories. The continuous domain example classifies irises (flower) into various sub-species.

30) Russell Reed and Robert J. Marks II, ``Neurosmithing: Improving Neural Network Learning,'' in \emph{The Handbook of Brain Theory and Neural Networks}, 1st ed., Cambridge: The MIT Press, pp. 639-644.
   Authors liken neural network training via backpropagation to placing a marble on a "...landscape with peaks and valleys..." and allowing it to roll into a local minimum. They point out this could lead to sub-optimal output and it is time consuming. They discuss how initial weights tend to be chosen and how that affects the learning rate, discuss how to normalize input, and review literature of other methods to speed up the training and generalization phases of neural network use.

31) Grossberg, S. Adaptive Resonance Theory: How a brain learns to consciously attend, learn, and recognize a changing world. Neural Networks (2012), doi:10.1016/j.neunet.2012.09.017.
   Citation intentionally deviates from IEEE format (request in footnote of article). Grossberg reviews current and previous advancements in ART. The power beyond ART resides in its ability to perform unsupervised and supervised learning. Article is beyond a paywall, but accessible through MST's library.

32) V.V. Tolat and B. Widrow, ``An Adaptive Neural Net Controller with Visual Inputs,'' Neural Networks, 1:362, Supplement 1, 1988. 
   This supplement builds on the Broom-Balancing article written by B. Widrow and F.W. Smith. Instead of directly measuring cart position, cart velocity, etc, as inputs to the neural network, the authors describe how binary pixels can also serve as inputs. In effect, the neural networks learns how to behave based on what it "sees". The images presented to the network are separated by fixed time samples and the authors claim that, much as the original broom balancing machine was able to maintain a stable pendulum position, the visual inputs allow the network to achieve the same goal. Article ends with a nod toward the "adaptive computer" where, instead of being programmed, the machine would be modeled in order to solve specific problems.

33) E. Wan, G. Kovacs, J. Rosen and B. Widrow, ``Development of Neural Network Interface for Direct Control of Neuroprostheses,'' Proceedings of the 1990 Meeting of International Neural Network Society and IEEE (IJCNN), 2:3-21, January 1990. 
   Fascinating paper demonstrating how far along neuroprostheses has come. Pictures showing surgical implants with actual axons growing through allow signal lines very similar to the inputs/outputs of an adaline. The article discusses reverse-engineering the signals coming from an amputee's brain in such a way that a Hopfield neural network could control a prosthetic attachment in a similar way to how the patient's brain controlled the original limb. Article reads largely like a philosophical discussion.

34) D. Nguyen and B. Widrow, ``Improving the Learning Speed of 2-Layer Neural Networks by Choosing Initial Values of the Adaptive Weights,'' Proceedings of the International Joint Conference on Neural Networks (IJCNN), 3:21-26, June 1990. 
   Article opens with a mathematical definition of a two-layer neural netowrk. Discussion of how to select initial weights follows and the article models a 2-layer neural network's output as a piece-wise linear segment union of the function it is supposed to approximate.

35) B. Widrow and M.A. Lehr, ``Adaptive Neural Networks and their Applications,'' International Journal of Intelligent Systems, 8(4):453-507, April 1993. 
   Reviews various neural network approaches. Starts off with linear combiners (adalines) and then describes non-linear combining systems, main adalines with pre-processed inputs. The article gives in-depth review of different learning strategies (madaline rule I and II, perceptron, etc) and gives examples towards the end. In general, the examples can be found in other literature by Widrow. Would serve admirably as an introudction paper to someone with Calc III and no CI experience.

36) B. Widrow, D. Rumelhart, and M.A. Lehr, ``Neural Networks: Applications in Industry, Business, and Science,'' Communications of the ACM, 37(3):93-105, March 1994. 
   Article reads like a laundry list of present and near applications of neural networks in a wide variety of areas. Would serve well as an example of "why should I learn this?" answers. Gives examples of linear and non-linear working examples. In the conclusion, the author notes that problems involving uncertainty (like Online Character Recognition) are well-served by application of a neural network.

37) R. Gazit and B. Widrow, ``Backpropagation Through Links: A New Approach to Kinematic Control of Serial Manipulators,'' Proceedings of the IEEE Internaional Symposium on Intelligent Control, pp.99-104, August 1995. 
   Application of Backpropagation Through Time to a robotic arm controller; solves a specific Inverse Kinematic Problem using neural networks but without known solutions being presented to the controller. The key insight appears to be defining all kinematic angles relative to the same origin line.

38) B. Widrow and G. Plett, "`Intelligent' Adaptive Inverse Control," Proceedings of IFAC, pp.104-105, July 1996. 
   Describes the basic concept of using a neural network to control a dynamic system (referred to as a plant in the article). Gives a schematic of a neural network controlling for noise by use of a model. Largely, the article is a dicussion and describes this form of control as an open research area.`

39) M.M. Lamego, E.P. Ferreira and B. Widrow, ``Neurointerfaces: Learning by Genetic Algorithms,'' Proceedings of the 14th World IFAC Congress, K:13-19, July 1999. 
   Article describes how to train a nerual interface and likens the control required to the inverse model mentioned in "`Intelligent' Adaptive Inverse Control. Also briefly mentions how genetic algorithms can be used to train the neural network. It should be noted that a neural interface in the context of this paper refers to the interface between a human being and a neural network. The neurointerface simplifies the problem on which the human attends.

40) B. Widrow and M.M. Lamego, ``Neurointerfaces: Principles,'' Proceedings of the IEEE Symposium on Adaptive Systems for Signal Processing, Communications, and Control, pp.315-320, October 2000. 
   Provides a clear neurointerface definition: "...a form of inverse of the plant to be controlled." Article uses the truck backer-upper as an example of a non-linear system and describes what a neurointerface to a two trailer semi-truck would look like, how you would train it, and how you would control it. Also discusses disturbance and how to address it.

41) B. Widrow and M.M. Lamego, ``Neurointerfaces: Applications,'' Proceedings of the IEEE Symposium on Adaptive Systems for Signal Processing, Communications and Control, pp.441-444, October 2000. 
   Reads like a continuation of ``Neurointerfaces: Principles''. Specifically details the two-trailer semi truck and includes not only a notional example, but also a physical scale-model with sample output. Authors also apply their neurointerface principles to a construction crane, providing a detailed model and mentions that in a real application disturbance would have to be accounted for.

42) C.-Y. Seong and B. Widrow, ``Neural Dynamic Optimization for Control Systems; I Background,'' IEEE Transactions on Systems, Man and Cybernetics, Part B, 31(4):482-489, August 2001. 
   Lots of acronyms. Paper presents how to apply Neural Dynamic Optimizations (NDO) to Multi-Input Multi-Output systems (MIMOs) starting by reviewing Dynamic Programming (DP), the Linear Quadratic Rule (LQR) and Feedforward Optimal Control (FOC). Specifically, the math behind these three stable control algorithms are reviewed followed by a brief discussion of how Neural Networks will apply.

43) C.-Y. Seong and B. Widrow, ``Neural Dynamic Optimization for Control Systems; II Theory,'' IEEE Transactions on Systems, Man and Cybernetics, Part B, 31(4):490-501, August 2001. 
   Defines and applies Neural Dynamic Optimization (NDO) to the same class of problems as Dynamic Programming. Compares the Dynamic Programming algorithm to the Feedforward Optimal Control algorithm and then equates NDO to FOC. Very math heavy paper proving each step.

44) C.-Y. Seong and B. Widrow, ``Neural Dynamic Optimization for Control Systems; III Applications,'' IEEE Transactions on Systems, Man and Cybernetics, Part B, 31(4):502-513, August 2001. 
   Applies Neural Dynamic Optimization (defined in the previous two articles read, NDO) to a Boeing 747 time-invariant model. The model proves that NDO can stabilize the lateral motion of a notional 747 (lateral motion is unstable due to the eigenvalues of the state-space matrix). The paper also models backing a truck with two trailers, similar to previous Widrow research, and solves the Iverse Kinematic Problem using NDO. Each example discusses how the models are constructed and provides many graphs of how each variable affects the model and is stabilized and solved.
